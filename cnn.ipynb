{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Toxic comment classification** CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers, initializers, optimizers, callbacks\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import *\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 200000    # max no. of words for tokenizer\n",
    "MAX_SEQUENCE_LENGTH = 400 # max length of each entry (sentence), including padding\n",
    "VALIDATION_SPLIT = 0.2   # data for validation (not used in training)\n",
    "EMBEDDING_DIM = 100      # embedding dimensions for word vectors (GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(frac=1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = int(VALIDATION_SPLIT*data.shape[0])\n",
    "x_train = data[: -num_test_samples]\n",
    "y_train = labels[: -num_test_samples]\n",
    "x_test = data[-num_test_samples: ]\n",
    "y_test = labels[-num_test_samples: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((127657, 6), (31914, 6))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = x_train['comment_text']\n",
    "X_test = x_test['comment_text'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "counting how many words are there in each type of toxic comment \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each count in train:  [12257  1284  6780   386  6295  1100]\n",
      "each count in test:  [3037  311 1669   92 1582  305]\n"
     ]
    }
   ],
   "source": [
    "print('each count in train: ', y_train.sum(axis=0))\n",
    "print('each count in test: ', y_test.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### removing stop words, punctuations, making lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(lambda x: x.strip().lower())\n",
    "X_train = X_train.apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "def remove_stopwords(line):\n",
    "    clean_words = [word for word in line.split() if word not in stopWords]\n",
    "    return ' '.join(clean_words)\n",
    "X_train = X_train.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.apply(lambda x: x.strip().lower())\n",
    "X_test = X_test.apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "def remove_stopwords(line):\n",
    "    clean_words = [word for word in line.split() if word not in stopWords]\n",
    "    return ' '.join(clean_words)\n",
    "X_test = X_test.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0         explanation edits made username hardcore metal...\n",
       " 1         daww matches background colour im seemingly st...\n",
       " 2         hey man im really trying edit war guy constant...\n",
       " 3         cant make real suggestions improvement wondere...\n",
       " 4                       sir hero chance remember page thats\n",
       "                                 ...                        \n",
       " 127652                                             well see\n",
       " 127653    speedy deletion 27 tricor ave new paltz ny 125...\n",
       " 127654    additions made 100 factual apparent youre anot...\n",
       " 127655    nathan thousand times seems let try explain ar...\n",
       " 127656    march 2007 utc imagemarist high school oregon ...\n",
       " Name: comment_text, Length: 127657, dtype: object,\n",
       " 127657    notice changed username accordance wiki polici...\n",
       " 127658    wp articles genealogical entries trees says wp...\n",
       " 127659                  redirect talkjohn rogers footballer\n",
       " 127660    nfl draft batch copyandpasting draft order tim...\n",
       " 127661    discussion copied wp reliable sources noticeboard\n",
       "                                 ...                        \n",
       " 159566    second time asking view completely contradicts...\n",
       " 159567       ashamed horrible thing put talk page 128611993\n",
       " 159568    spitzer umm theres actual article prostitution...\n",
       " 159569    looks like actually put speedy first version d...\n",
       " 159570    really dont think understand came idea bad rig...\n",
       " Name: comment_text, Length: 31914, dtype: object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data: daww matches background colour im seemingly stuck thanks talk 2151 january 11 2016 utc [0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print('Sample data:', X_train[1], y_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**oov_token** (if it ever sees a new word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, oov_token = True)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 213981\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "print('Vocabulary size:', len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 213981\n"
     ]
    }
   ],
   "source": [
    "sequences2 = tokenizer.texts_to_sequences(X_test)\n",
    "word_index2 = tokenizer.word_index\n",
    "print('Vocabulary size:', len(word_index2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[517,\n",
       " 44,\n",
       " 48,\n",
       " 512,\n",
       " 4356,\n",
       " 12081,\n",
       " 1155,\n",
       " 212,\n",
       " 1928,\n",
       " 10536,\n",
       " 6576,\n",
       " 2517,\n",
       " 2689,\n",
       " 37,\n",
       " 1021,\n",
       " 14712,\n",
       " 2651,\n",
       " 6,\n",
       " 10,\n",
       " 137,\n",
       " 309,\n",
       " 5,\n",
       " 3,\n",
       " 59,\n",
       " 14,\n",
       " 3457,\n",
       " 58446]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (127657, 400)\n",
      "Shape of label tensor: (127657, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = pad_sequences(sequences, padding = 'post', maxlen = MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (31914, 400)\n",
      "Shape of label tensor: (31914, 6)\n"
     ]
    }
   ],
   "source": [
    "data2 = pad_sequences(sequences2, padding = 'post', maxlen = MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of data tensor:', data2.shape)\n",
    "print('Shape of label tensor:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentences: \n",
      " [  517    44    48   512  4356 12081  1155   212  1928 10536  6576  2517\n",
      "  2689    37  1021 14712  2651     6    10   137   309     5     3    59\n",
      "    14  3457 58446     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "One hot label: \n",
      " [0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print('Tokenized sentences: \\n', data[0])\n",
    "print('One hot label: \\n', y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import max_norm\n",
    "from keras.layers import (\n",
    "    Input, Embedding, Dense, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout\n",
    ")\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**:\n",
    "\n",
    "Georgakopoulos, S. V., Tasoulis, S. K., Vrahatis, A. G., & Plagianakos, V. P. (2018, July). Convolutional neural networks for toxic comment classification. In Proceedings of the 10th Hellenic Conference on Artificial Intelligence (p. 35). ACM.\n",
    "\n",
    "Kim, Y. (2014). Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "# see Georgakopoulos et al. (2018)\n",
    "n_filters = 128\n",
    "dropout_rate = 0.5\n",
    "fc_dim = 300\n",
    "# l2_norm constraint, see Kim (2014)\n",
    "s = 3.\n",
    "\n",
    "learning_rate = 0.005\n",
    "batch_size = 64\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we get in (examples, words, embedding_size) tensor\n",
    "input_tensor = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "embedding_tensor = Embedding(\n",
    "    MAX_NB_WORDS,  # vocabulary size\n",
    "    EMBEDDING_DIM,  # dimension of dense embedding\n",
    "    input_length=MAX_SEQUENCE_LENGTH \n",
    ")(input_tensor)  # outputs (, seq_length, embeddin_dims) tensor\n",
    "\n",
    "# -- convolution block --\n",
    "block_1 = Conv1D(\n",
    "    n_filters, \n",
    "    kernel_size=3,\n",
    "    padding='valid',\n",
    "    activation='relu',\n",
    "    strides=1,\n",
    "    kernel_constraint=max_norm(s)\n",
    ")(embedding_tensor)  # output is (batch, new_steps, filters)\n",
    "# max over time pooling\n",
    "block_1 = GlobalMaxPooling1D()(block_1)  # output is tensor of shape (batch, filters)\n",
    "\n",
    "block_2 = Conv1D(\n",
    "    n_filters, \n",
    "    kernel_size=4,\n",
    "    padding='valid',\n",
    "    activation='relu',\n",
    "    strides=1,\n",
    "    kernel_constraint=max_norm(s)\n",
    ")(embedding_tensor)\n",
    "block_2 = GlobalMaxPooling1D()(block_2)\n",
    "\n",
    "block_3 = Conv1D(\n",
    "    n_filters, \n",
    "    kernel_size=5,\n",
    "    padding='valid',\n",
    "    activation='relu',\n",
    "    strides=1,\n",
    "    kernel_constraint=max_norm(s)\n",
    ")(embedding_tensor)\n",
    "# max-over-time pooling\n",
    "block_3 = GlobalMaxPooling1D()(block_3)\n",
    "\n",
    "\n",
    "# -- fully-connected block --\n",
    "# concatenate results of into tensor of shape (batch, filters + filters + filters)\n",
    "concat = Concatenate()([block_1, block_2, block_3])\n",
    "# dropout\n",
    "concat = Dropout(dropout_rate)(concat)\n",
    "\n",
    "fc = Dense(\n",
    "    fc_dim,\n",
    "    activation='relu',\n",
    "    kernel_constraint=max_norm(s)\n",
    ")(concat)\n",
    "fc = Dropout(dropout_rate)(fc)\n",
    "\n",
    "# fully-connected softmax layer with l2 regularization\n",
    "predictions = Dense(\n",
    "    6,\n",
    "    activation='sigmoid',\n",
    "    kernel_constraint=max_norm(s)\n",
    ")(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 400)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 400, 100)     20000000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 398, 128)     38528       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 397, 128)     51328       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 396, 128)     64128       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 128)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 384)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 384)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 300)          115500      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 300)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 6)            1806        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 20,271,290\n",
      "Trainable params: 20,271,290\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# This creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "model = Model(inputs=input_tensor, outputs=predictions)\n",
    "\n",
    "\n",
    "adam = Adam(lr=learning_rate)\n",
    "model.compile(optimizer=adam,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((127657, 400), (127657, 6))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aj/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/1\n",
      "127657/127657 [==============================] - 1095s 9ms/step - loss: 0.0742 - accuracy: 0.9758 - val_loss: 0.0570 - val_accuracy: 0.9799\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "history = model.fit(\n",
    "    data, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(data2, y_test)\n",
    ")\n",
    "elapsed_pa = timeit.default_timer() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1096.59785493\n"
     ]
    }
   ],
   "source": [
    "print(elapsed_pa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss #for multiclass\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming_loss : 2.0064339579286\n",
      "Accuracy : 91.11675126903553\n"
     ]
    }
   ],
   "source": [
    "predict = np.round(predict)\n",
    "loss = hamming_loss(y_test,predict)\n",
    "print(\"Hamming_loss : {}\".format(loss*100))\n",
    "accuracy = accuracy_score(y_test,predict)\n",
    "print(\"Accuracy : {}\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
